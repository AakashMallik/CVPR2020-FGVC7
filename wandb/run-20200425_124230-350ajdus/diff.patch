diff --git a/README.md b/README.md
index 416433d..1cc3c8b 100644
--- a/README.md
+++ b/README.md
@@ -68,12 +68,21 @@ The backbones we are looking forward to explore are:
     - [x] Stratified k fold
 - [x] Models
     - [x] Efficientnet B7
+    - [x] Efficientnet B4
+    - [x] Resnet 34
+    - [x] Densenet 161
 - [x] Loss
     - [x] Cross Entropy Loss
     - [x] Focal loss
-    - [x] Mean ROC AUC loss ( Kaggle )
+    - [x] ArcFace loss
+    - [ ] Mean ROC AUC loss ( Kaggle )
 - [x] Optimizer
     - [x] RMS prop with Efficient net params
+    - [X] Adam
+    - [x] AdamW
+- [X] Scheduler
+    - [X] CosineAnelingLR
+    - [X] StepLR
 
 ---
 
@@ -134,10 +143,12 @@ train_dataset:
   name:
   fold: ( optional )
   resize_dims:
+  transform: (optional, uses default if not specified)
 val_dataset: 
   name:
   fold: ( optional )
   resize_dims:
+  transform: (optional, uses default if not specified)
 model: 
   name:
   pred_type: regression/classification/mixed
@@ -184,6 +195,26 @@ experiment_list:
       weight_type: ( optional key to pick the type of weight - best_val_loss/best_val_roc )
 ```
 
+## How to use
+#### 1. Using it for train/test
+simply modify the run.sh with the required commands:  
+```
+python main.py "train_dummy_1.yml"
+python main.py "test_dummy.yml"
+```  
+  
+#### 2. Using it for augmentation search
+Add ```-a``` flag with the training config that you are going to use for scoring:
+```
+python main.py -a 'train_dummy_1.yml'
+```  
+  
+#### 3. Publish mode while training
+Add ```-p``` flag with the training config to publish results to W&B and Telegram:
+```
+python main.py -p 'train_dummy_1.yml'
+```
+
 ## Lib dependency
 ```bash
 pip install --user torch wandb torchvision catalyst albumentations pandas scikit-image tqdm scikit-learn pyyaml blessed pyzmq pretrainedmodels tensorboard
diff --git a/models/__pycache__/utils.cpython-37.pyc b/models/__pycache__/utils.cpython-37.pyc
index 43faa6e..3229b6d 100644
Binary files a/models/__pycache__/utils.cpython-37.pyc and b/models/__pycache__/utils.cpython-37.pyc differ
diff --git a/models/model_factory.py b/models/model_factory.py
index ad5d5c9..00cabbb 100644
--- a/models/model_factory.py
+++ b/models/model_factory.py
@@ -52,6 +52,23 @@ class ModelFactory():
             if hyper_params is not None:
                 model._bn_mom = hyper_params['batch_norm_momentum']
 
+        if model_name == 'efficientnet-b5':
+            print("[ Model : Efficientnet B5 ]")
+            model = EfficientNet.from_pretrained(
+                model_name='efficientnet-b5', advprop=True)
+            if tuning_type == 'feature-extraction':
+                for param in model.parameters():
+                    param.requires_grad = False
+            num_ftrs = model._fc.in_features
+            model._fc = nn.Sequential(
+                # nn.Linear(num_ftrs, num_classes),
+                # nn.Sigmoid(),
+                nn.Linear(num_ftrs, adjusted_num_classes)
+            )
+
+            if hyper_params is not None:
+                model._bn_mom = hyper_params['batch_norm_momentum']
+
         if model_name == 'densenet-161':
             print("[ Model : Densenet 161 ]")
             model = models.densenet161(pretrained=True)
@@ -65,7 +82,8 @@ class ModelFactory():
 
         if model_name == 'resnet34':
             print("[ Model : Resnet 34 ]")
-            model = pretrainedmodels.__dict__['resnet34'](pretrained='imagenet')
+            model = pretrainedmodels.__dict__[
+                'resnet34'](pretrained='imagenet')
 
             model.avgpool = nn.AdaptiveAvgPool2d(1)
             in_features = model.last_linear.in_features
diff --git a/models/utils.py b/models/utils.py
index 36a3ddf..dd6c465 100644
--- a/models/utils.py
+++ b/models/utils.py
@@ -299,6 +299,11 @@ def load_pretrained_weights(model, model_name, load_fc=True, advprop=False):
     state_dict = None
     if model_name == 'efficientnet-b7':
         state_dict = torch.load("pretrained_weights/efficientnet_b7.pth")
+    elif model_name == 'efficientnet-b5':
+        if advprop:
+            state_dict = torch.load("adv_pretrained_weights/efficientnet_b5.pth")
+        else:
+            state_dict = torch.load("pretrained_weights/efficientnet_b5.pth")
     elif model_name == 'efficientnet-b4':
         state_dict = torch.load("pretrained_weights/efficientnet_b4.pth")
     
diff --git a/run.sh b/run.sh
index a270ff0..01d334a 100755
--- a/run.sh
+++ b/run.sh
@@ -1,5 +1,5 @@
 # Dummy
-python ./main.py -a "train_dummy_1.yml"
+# python ./main.py -a "train_dummy_1.yml"
 # python ./main.py "train_dummy_2.yml"
 
 # python ./main.py "test_dummy.yml"
@@ -78,4 +78,7 @@ python ./main.py -a "train_dummy_1.yml"
 # python ./main.py -p 'exp_19_c.yml'
 # python ./main.py -p 'exp_19_d.yml'
 # python ./main.py -p 'exp_19_e.yml'
-# python ./main.py 'test_19.yml'
\ No newline at end of file
+# python ./main.py 'test_19.yml'
+
+python ./main.py -p 'exp_20_a.yml'
+python ./main.py -p 'exp_20_b.yml'
\ No newline at end of file
diff --git a/utils/__pycache__/experiment_utils.cpython-37.pyc b/utils/__pycache__/experiment_utils.cpython-37.pyc
index 0afb739..e93aa91 100644
Binary files a/utils/__pycache__/experiment_utils.cpython-37.pyc and b/utils/__pycache__/experiment_utils.cpython-37.pyc differ
diff --git a/utils/experiment_utils.py b/utils/experiment_utils.py
index 1addb5d..e942fe6 100644
--- a/utils/experiment_utils.py
+++ b/utils/experiment_utils.py
@@ -75,11 +75,13 @@ class ExperimentHelper:
                 state_dict,
                 path.join('results', self.experiment_name, 'weights_loss.pth')
             )
+            print("[Saved best Loss]", self.best_val_loss)
         if self.progress_roc:
             torch.save(
                 state_dict,
                 path.join('results', self.experiment_name, 'weights_roc.pth')
             )
+            print("[Saved best roc]", self.best_val_roc)
 
     def validate(self, pred_type, num_classes, loss_fn, val_output_list, val_target_list, train_output_list, train_target_list, epoch):
         with torch.no_grad():
diff --git a/wandb/debug.log b/wandb/debug.log
index f9f3c11..21eb7d6 100644
Binary files a/wandb/debug.log and b/wandb/debug.log differ
